{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1cfe23d3-26e4-4905-b370-68aa61c0dee9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Building the Evaluation Agent\n",
    "For full documentation on building AI agents in code, [refer to this doc](https://docs.databricks.com/aws/en/generative-ai/agent-framework/author-agent?language=LangGraph).\n",
    "\n",
    "This notebook is developed on DBR 16.4 ML LTS. At the time of writing, 17.3 ML LTS is having issues with Spark Connect and the default client mode. Tested in 17.2 and this works fine. This is because as of 11-Nov-2025 my region (aws us-west-2) only has serverless runtime 17.2 installed. Running DBR 17.3 causes a runtime mis-match error. Until my serverless plane is updated to 17.3, I have to use a slightly older DB Runtime. This is separate from running a notebook in serverless. This has to do with Databricks Connect in the backend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8fcf8f8-0af8-451e-a951-024e76b68689",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# DEMO START\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59e64d00-791b-4b96-af7e-a20aeef72fd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Dependencies\n",
    "Creating an agent requires the latest version of MLFlow (>=3.1.3), Python 3.10 or newer (default on serverless) as well as the Databricks agent framework and the langchain AI Bridge. Since ChatAgent has been deprecated, we will need to use OpenAI's `ResponsesAgent` framework for an MLFlow-compliant interface :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c5f2708-ad36-474a-9ebc-763d664ea472",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Use -qqq to show any errors or -qqqq for very quiet\n",
    "%pip install -U -qqq langgraph uv databricks-agents databricks-langchain mlflow-skinny[databricks] python-pptx python-docx\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4dd8082-d724-4bdb-830b-7bd6884515a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ResponsesAgent Overview\n",
    "It's important to note that `ResponsesAgent` is a wrapper to seamless interface with a variety of different agents in a common way. This means that we can author agents in Databricks, but have them interface with any platform. We will be using ResponsesAgent as a wrapper for our node agents and our supervisors.\n",
    "<br/>\n",
    "<br/>\n",
    "<img src=\"https://docs.databricks.com/aws/en/assets/images/responses-agent-overview-611d843718bf94974d277a365695043c.svg\" width=1000 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db2d3de2-6775-43d4-9d02-347a5049a36a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %%writefile -a eval_agent.py\n",
    "from typing import Annotated, Any, Generator, Optional, Sequence, TypedDict, Union, TypedDict, List, Dict\n",
    "\n",
    "import mlflow\n",
    "from databricks_langchain import (\n",
    "    ChatDatabricks,\n",
    "    UCFunctionToolkit,\n",
    "    VectorSearchRetrieverTool,\n",
    "    # DatabricksFunctionClient\n",
    ")\n",
    "from langchain_core.messages import AIMessage, AIMessageChunk, AnyMessage\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "# from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "# from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt.tool_node import ToolNode\n",
    "\n",
    "from mlflow.pyfunc import ResponsesAgent\n",
    "from mlflow.types.responses import (\n",
    "    ResponsesAgentRequest,\n",
    "    ResponsesAgentResponse,\n",
    "    ResponsesAgentStreamEvent,\n",
    "    output_to_responses_items_stream,\n",
    "    to_chat_completions_input,\n",
    ")\n",
    "\n",
    "from unitycatalog.ai.core.databricks import DatabricksFunctionClient\n",
    "\n",
    "from pptx import Presentation\n",
    "from docx import Document\n",
    "\n",
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6b0e803-7110-4094-b20c-181169401d0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create the Evaluation Struct (Object Schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26077572-89a9-48c3-834e-89a81625205b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %%writefile -a eval_agent.py\n",
    "class EvalState(TypedDict, total=False):\n",
    "    file_name: str                      # e.g. \"incident_123.pptx\"\n",
    "    user_prompt: str                    # any prompt / focus area text\n",
    "    pptx_chunks: List[str]              # parsed slide texts\n",
    "    docx_chunks: List[str]              # parsed paragraph texts\n",
    "    corrective_actions_raw: str         # raw LLM output from gen agent\n",
    "    corrective_actions: List[Dict]      # parsed actions from gen agent\n",
    "    evaluated_actions_raw: str          # raw LLM output from eval agent\n",
    "    evaluated_actions: List[Dict]       # top-k ranked actions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65f9d112-62cd-4161-9351-dcf970e64b7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## LM Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ad4aef7-ef75-4d74-8da4-79846e4e8aee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# #Define the endpoint to use for the agent foundation and system prompt\n",
    "LLM_ENDPOINT_NAME = \"databricks-claude-3-7-sonnet\"\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME)\n",
    "system_prompt = \"You are a tool used to create and rank corrective actions based on incident reports. You accept documents and user prompts to help understand what had occured and provide a recommendation on the top corrective actions to remediate and prevent similar incidents in the future.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b085d9d7-b0c2-4268-ae0a-57943aad5f4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create our Tools\n",
    "The docstring is really important since it determines tool routing by the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e58a44e3-5c30-4dd9-9c36-12620955e6e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %%writefile -a eval_agent.py\n",
    "UPLOAD_VOLUME = \"/Volumes/ademianczuk/suncor_ehs/data/uploads\"\n",
    "\n",
    "@tool(\"parse_pptx\")\n",
    "def parse_pptx_tool(file_name: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Parse a PPTX file stored in the UC Volume into slide-level text chunks. Use this to parse a Microsoft Powerpoint (.pptx) file.\n",
    "    \n",
    "    Args:\n",
    "        file_name: The PPTX filename (e.g. 'incident_123.pptx'), assumed to live under\n",
    "                   /Volumes/ademianczuk/suncor_ehs/data/uploads.\n",
    "\n",
    "    Returns:\n",
    "        A list of text chunks (strings). Each chunk is roughly slide-level; \n",
    "        small slides may be merged into bigger chunks to keep context.\n",
    "    \"\"\"\n",
    "    dbfs_path = f\"dbfs:{UPLOAD_VOLUME}/{file_name}\"\n",
    "    local_path = \"/tmp/input.pptx\"\n",
    "    dbutils.fs.cp(dbfs_path, f\"file:{local_path}\", True)\n",
    "\n",
    "    prs = Presentation(local_path)\n",
    "    chunks: list[str] = []\n",
    "\n",
    "    for slide_idx, slide in enumerate(prs.slides, start=1):\n",
    "        texts = []\n",
    "        for shape in slide.shapes:\n",
    "            if hasattr(shape, \"text\"):\n",
    "                txt = shape.text.strip()\n",
    "                if txt:\n",
    "                    texts.append(txt)\n",
    "        if texts:\n",
    "            #each slide = one chunk for now\n",
    "            chunks.append(f\"Slide {slide_idx}:\\n\" + \"\\n\".join(texts))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "@tool(\"parse_docx\")\n",
    "def parse_docx_tool(file_name: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Parse a DOCX file stored in the UC Volume into text chunks. Use this to parse a Microsoft Word document (.docx).\n",
    "\n",
    "    Args:\n",
    "        file_name: The DOCX filename (e.g. 'incident_123.docx'), assumed to live under\n",
    "                   /Volumes/ademianczuk/suncor_ehs/data/uploads.\n",
    "\n",
    "    Returns:\n",
    "        A list of text chunks (strings). Each chunk is roughly paragraph-level; \n",
    "        small paragraphs may be merged into bigger chunks to keep context.\n",
    "    \"\"\"\n",
    "\n",
    "    dbfs_path = f\"dbfs:{UPLOAD_VOLUME}/{file_name}\"\n",
    "    local_path = \"/tmp/input.docx\"\n",
    "    dbutils.fs.cp(dbfs_path, f\"file:{local_path}\", True)\n",
    "\n",
    "    doc = Document(local_path)\n",
    "    chunks: List[str] = []\n",
    "    current = \"\"\n",
    "    max_chars = 1200 #tune as required\n",
    "\n",
    "    paragraphs: List[str] = []\n",
    "    for para in doc.paragraphs:\n",
    "        text = para.text.strip()\n",
    "        if not text:\n",
    "            continue\n",
    "        paragraphs.append(text)\n",
    "\n",
    "    for p in paragraphs:\n",
    "        #If adding the next paragraph would exceed max_chars, create a new chunk\n",
    "        if len(current) + len(p) + 2 > max_chars:\n",
    "            if current:\n",
    "                chunks.append(current.strip())\n",
    "            current = p\n",
    "        else:\n",
    "            if current:\n",
    "                current += \"\\n\\n\" + p\n",
    "            else:\n",
    "                current = p\n",
    "\n",
    "    if current:\n",
    "        chunks.append(current.strip())\n",
    "\n",
    "    return chunks\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d9a4d9c-dc28-4cf8-ab9f-dd9339d6a25d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Define the State Container Object\n",
    "Since we need to preserve the state of the agent through conversation turns, we need to create a struct that defines the schema for the object. This is what handles keeping track of the messaging as the agent is updated. In other words, this keeps all of the agent messages organized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd5be8dc-004f-4840-9979-d540ab4145be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %%writefile -a agent.py\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[AnyMessage], add_messages]\n",
    "    custom_inputs: Optional[dict[str, Any]]\n",
    "    custom_outputs: Optional[dict[str, Any]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c8af782-dc08-4a7b-8b72-510ca4338427",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Tool Calling Agent Core Logic\n",
    "**NOTE!** Because we're using the MLFLow / Databricks implmentation of LangChain, we don't have access to the newer LangChain capabilities. With upcoming releases of Databricks LangChain, the `create_tool_calling_agent()` function will be available as part the langchain agents library. In the meantime, we can add it inline here to take advantage of the placeholder logic knowing full-well this is coming as a native implementation soon.\n",
    "\n",
    "For now (as of November 14, 2025) we'll use this boilerplate function. In the future we can just create instances `create_tool_calling_agent()` and `AgentExecutor()` as part of the langchain.agents library which will get rolled into the Databricks LangChain implementation soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7417421-dc08-4751-936f-03d5e9992301",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %%writefile -a agent.py\n",
    "def create_tool_calling_agent(\n",
    "    model: ChatDatabricks,\n",
    "    tools: Union[ToolNode, Sequence[BaseTool]],\n",
    "    system_prompt: Optional[str] = None,\n",
    "):\n",
    "    model = model.bind_tools(tools)\n",
    "\n",
    "    # Define the function that determines which node to go to\n",
    "    def should_continue(state: AgentState):\n",
    "        messages = state[\"messages\"]\n",
    "        last_message = messages[-1]\n",
    "        # If there are function calls, continue. else, end\n",
    "        if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "            return \"continue\"\n",
    "        else:\n",
    "            return \"end\"\n",
    "\n",
    "    if system_prompt:\n",
    "        preprocessor = RunnableLambda(\n",
    "            lambda state: [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "        )\n",
    "    else:\n",
    "        preprocessor = RunnableLambda(lambda state: state[\"messages\"])\n",
    "    model_runnable = preprocessor | model\n",
    "\n",
    "    def call_model(\n",
    "        state: AgentState,\n",
    "        config: RunnableConfig,\n",
    "    ):\n",
    "        response = model_runnable.invoke(state, config)\n",
    "\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    workflow = StateGraph(AgentState)\n",
    "\n",
    "    workflow.add_node(\"agent\", RunnableLambda(call_model))\n",
    "    workflow.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "    workflow.set_entry_point(\"agent\")\n",
    "    workflow.add_conditional_edges(\n",
    "        \"agent\",\n",
    "        should_continue,\n",
    "        {\n",
    "            \"continue\": \"tools\",\n",
    "            \"end\": END,\n",
    "        },\n",
    "    )\n",
    "    workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "    return workflow.compile()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dcb21cbf-380e-46aa-ac18-73891272cc52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Corrective Actions Agent Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beae9ef6-fc32-4ff1-b1e4-c576afaccb8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CORRECTIVE_ACTIONS_INSTRUCTIONS = \"\"\"\n",
    "You are a corporate Enterprise Health and Safety (EHS) expert specializing in corrective actions.\n",
    "\n",
    "With the following guidance, define and create corrective actions:\n",
    "\n",
    "1. Before reading the remaining instructions below, you must observe all uploaded documents and ensure that you have a full and comprehensive understanding of them. You must ensure that you are able to extrapolate from the provided resources, directly following the framework, definitions, and standards provided in doing so. Make sure that you are looking for pattern recognition and defining areas of interest that are related to the goal trying to be achieved.\n",
    "\n",
    "2. You will be provided with an incident investigation report. This may be presented in a PDF, Word, or PowerPoint format and you must be ready to read, interpret, and understand the content provided by the user. Your goal is to flag the report for instances of 'negative' reasoning. The definitions for these terms are provided in the uploaded document labelled 'Consolidated Context Format.docx'. Additionally, you must also identify the presence of counterfactuals in the report provided. In carrying out these defined tasks, you must also determine the difference between negative and causal reasoning as you will need this for analysis.\n",
    "\n",
    "3. When finding instances of negative reasoning, showcase how you would define and extract instances of negative reasoning for future inquiries. You must determine what factors constitute the usage of negative reasoning in any given incident investigations report.\n",
    "\n",
    "4. You must structure the output as follows. I want you to have three headers: 'Relationship', 'Flagging', and 'Corrective Actions'.\n",
    "\n",
    "5. Under 'Relationship', reassess the prompt provided by the user, as well as the uploaded document labelled 'Consolidated Context Format.docx'. Review the incident investigation report and specifically identify any terms and definitions from the uploaded documents that are present. Identify specific excerpts of the investigation report that contain any terms and definitions. Explain how they are present. Do not be vague and try to find the presence of as many terms and definitions as you can with sufficient detail. Do not combine any definitions.\n",
    "\n",
    "6. Under 'Flagging', specify all instances of negative reasoning in the report, as well as the presence of counterfactuals and any logical errors. Put these into a table with four columns, 'Identification of Negative Reasoning / Counterfactual' (Identify whether you are assessing an instance of only 'Negative Reasoning' or a 'Counterfactual', or combine both together if a certain excerpt has both an instance of negative reasoning and a counterfactual), 'Identification Reasoning' (How did you identify a given instance of negative reasoning or the usage of a counterfactual? What factors revealed their presence?), 'Definitions Present' (Which of the definitions above are present in the excerpt extracted?), and 'Original Statement' (The original statement being analyzed). Ensure that you are specific and detailed in every column.\n",
    "\n",
    "7. Under 'Corrective Actions',\n",
    "\n",
    "I want you to list the causes and actions from the investigation report in a sentence format above the table. I then want you to create a table that is populated with every single cause and action in the report from the initial list you made, in the order specified under the forthcoming column called 'Quality of Action'. Before creating the table with the following columns, ensure the initial lists' actions are ranked according to the 'Quality of Action' column below before the table is created from the list. The table should be structured as follows with the following logic:\n",
    "    - 'Tally' (Just increments and counts every row)\n",
    "    - 'Action' (the corresponding corrective action)\n",
    "    - 'Cause' (The initial cause that the action is a result of)\n",
    "    - 'OEMS Process' (the associated and most applicable OEMS Process defined in the document labelled 'OEMS Process Descriptions.docx' to the cause)\n",
    "    -  'Related OEMS Process' (to showcase all other most applicable OEMS Processes (more than one) from the document labelled 'OEMS Process Descriptions.docx')\n",
    "    - 'Hierarchy of Controls' (which determines the control hierarchy level the corrective action best embodies, defined in the document labelled 'Consolidated Context Format.docx' under the section titled 'Hierarchy of Controls - Corrective Actions')\n",
    "    - 'Quality of Action (which ranks the action based on what control hierarchy level it is at (create a scale where Elimination would be a 5 (Most Effective), Substitution would be a 4 (Highly Effective), Engineering Controls would be a 3 (Effective), Administrative Controls would be a 2 (Less Effective), and PPE would be a 1 (Least Effective) PPE would be a 1 (Least Effective)), formatted exactly as defined)\n",
    "    - 'Top Three Actions' (It chooses the top three actions that have the best Root Cause Identification (Utilize Causal analysis to find underlying causes, not just symptoms), Break the Causal Chain (Implement controls at multiple points; use redundancy), are Systemic and Sustainable (Focus on process, policy, and resource improvements for long-term impact), are Specific and Measurable (Define clear actions, responsibilities, and metrics for success), and reflect viable Verification and Improvement (Monitor, audit, and refine actions for ongoing effectiveness). From there, the column is populated with its ranking number and a very detailed, long, and specific (very specific to the action, including many details from the action) justification, and anything under 3 is left blank. The best corrective actions are those that address the underlying root causes of an incident through a thorough causal analysis, rather than just treating immediate symptoms. They break the chain of events at multiple points by implementing layered controls, engineering, administrative, and behavioral, to ensure redundancy and resilience. Effective corrective actions are systemic and sustainable, focusing on long-term improvements to processes, policies, and resource allocation. They are also specific, clearly defining responsibilities, timelines, and measurable outcomes, and are verified for effectiveness through ongoing monitoring and continuous improvement. This comprehensive approach ensures that corrective actions not only resolve the current issue but also prevent recurrence and strengthen organizational safety and reliability)\n",
    "    - 'Assessment of Effectiveness' (For each action, what would be a good criterion to determine whether its implementation was effective?. This focuses on how effective the action is in achieving the desired end. You must explore that if the same cause reoccurred, how would the presence of the action alter the timeline of events, and what criterion would be used to evaluate its effectiveness. Clearly define the specific criteria and measurable outcomes that will be used to determine if the action is effective. Also describe the method of verification (e.g., physical testing, audit, scenario review, monitoring of performance indicators). Explain how ongoing monitoring and continuous improvement will be ensured (e.g., scheduled reviews, integration into lessons learned, feedback loops). And finally, explore how the action would disrupt the cause and effect chain in future cases, and what would be used to evaluate this disruption)\n",
    "Now that you are aware of how the table should be structured, begin by creating the list of causes and actions, with their respective 'Quality of Action' rankings, and then create the table. All table rows must be sorted strictly by the 'Quality of Action' column in descending order. All actions with '5 - Most Effective' must be at the very top, followed by '4 - Highly Effective', '3 - Effective', 2 - 'Less Effective', and 1 - 'Least Effective'. You must not mix, group, or list actions by any other order, and you must not list any lower-ranked action above a higher one. Do not preserve the original order from the report; only sort by 'Quality of Action'. After presenting the tables, you must write: \"All tables have been sorted by 'Quality of Action' in strict descending order.\" Before submitting your response, check every table and confirm that the sorting is correct. If any table is not sorted properly, you must fix it before submitting.\n",
    "\n",
    "8. After presenting the two tables above, you must write: \"All tables have been sorted by 'Quality of Action' in strict descending order.\" Before submitting your response, check every table and confirm that the sorting is correct. If any table is not sorted properly, you must fix it before submitting.\n",
    "\n",
    "9. Please do not add anything extra I did not ask you to add.\n",
    "\n",
    "10. Whenever I say to include 'all of' something. Include every single instance of what is being asked to be provided.\n",
    "\n",
    "11. Be very detailed.\n",
    "\n",
    "Your task:\n",
    "- Analyze the content of the incident/document.\n",
    "- Propose specific CORRECTIVE ACTIONS that address root causes and key risks.\n",
    "- Each action must be:\n",
    "  - Concrete and implementable.\n",
    "  - Clearly linked to a risk or failure in the incident.\n",
    "  - Framed in a professional corporate tone.\n",
    "\n",
    "Output MUST be valid JSON with this structure:\n",
    "{\n",
    "  \"corrective_actions\": [\n",
    "    {\n",
    "      \"id\": \"{{filename}}\",\n",
    "      \"title\": \"...\",\n",
    "      \"description\": \"...\",\n",
    "      \"risk_addressed\": \"...\",\n",
    "      \"root_cause_addressed\": \"...\",\n",
    "      \"owner_suggestion\": \"...\",\n",
    "      \"timeframe\": \"Short-term|Medium-term|Long-term\",\n",
    "      \"impact\": \"High|Medium|Low\",\n",
    "      \"confidence\": 0.0\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "Do not include any text outside the JSON.\n",
    "\"\"\"\n",
    "\n",
    "# corrective_actions_prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\"system\", CORRECTIVE_ACTIONS_INSTRUCTIONS),\n",
    "#         (\n",
    "#             \"user\",\n",
    "#             \"User focus/prompt:\\n{user_prompt}\\n\\n\"\n",
    "#             \"Here are the slide contents of the incident report:\\n\\n\"\n",
    "#             \"{pptx_text}\\n\\n\"\n",
    "#             \"Using the report above and the instructions, generate a set of corrective actions.\"\n",
    "#         ),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# gen_tools: list = []\n",
    "\n",
    "# corrective_actions_agent = create_tool_calling_agent(\n",
    "#     llm=llm,\n",
    "#     tools=gen_tools,\n",
    "#     prompt=corrective_actions_prompt,\n",
    "# )\n",
    "\n",
    "# corrective_actions_executor = AgentExecutor(\n",
    "#     agent=corrective_actions_agent,\n",
    "#     tools=gen_tools,\n",
    "#     verbose=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b246db65-b818-4ec0-a310-fb0277716ddd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Evaluation Agent Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1ccb094-dd1e-4905-98c9-e2c79c5f5638",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "EVALUATION_INSTRUCTIONS = \"\"\"\n",
    "You are an expert reviewer of EHS corrective actions.\n",
    "\n",
    "Your job:\n",
    "- Evaluate the provided corrective actions for their potential to significantly reduce risk and improve safety.\n",
    "- Consider:\n",
    "  - Breadth of risk reduction.\n",
    "  - Depth (severity) of issues addressed.\n",
    "  - Feasibility and clarity.\n",
    "  - Alignment with the customer's corrective-action guidelines.\n",
    "\n",
    "You MUST:\n",
    "- Select the TOP 3 corrective actions with the most significant impact.\n",
    "- Provide a short evaluation summary for each selected action.\n",
    "\n",
    "Output MUST be valid JSON:\n",
    "{\n",
    "  \"top_corrective_actions\": [\n",
    "    {\n",
    "      \"id\": \"...\",\n",
    "      \"title\": \"...\",\n",
    "      \"reason_for_selection\": \"...\",\n",
    "      \"expected_impact\": \"High|Medium\",\n",
    "      \"comments\": \"...\",\n",
    "      \"original_action\": { ... }  // copy of original action object\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "Do not include any text outside the JSON.\n",
    "\"\"\"\n",
    "\n",
    "# evaluation_prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\"system\", EVALUATION_INSTRUCTIONS),\n",
    "#         (\n",
    "#             \"user\",\n",
    "#             \"Here is the full list of corrective actions (JSON):\\n{corrective_actions_json}\\n\\n\"\n",
    "#             \"Select and return only the top 3 according to the instructions.\"\n",
    "#         ),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# evaluation_agent = create_tool_calling_agent(\n",
    "#     llm=llm,\n",
    "#     tools=[],\n",
    "#     prompt=evaluation_prompt,\n",
    "# )\n",
    "\n",
    "# evaluation_executor = AgentExecutor(\n",
    "#     agent=evaluation_agent,\n",
    "#     tools=[],\n",
    "#     verbose=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9452fcb-b396-4b35-b021-a5aaa26822ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc5df88b-d6c2-44a4-8d70-daf4943feeaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "def parse_pptx_node(state: EvalState) -> EvalState:\n",
    "    \"\"\"Node 0: parse pptx into chunks using the tool.\"\"\"\n",
    "    chunks = parse_pptx_tool.invoke(state[\"file_name\"])\n",
    "    return {\"pptx_chunks\": chunks}\n",
    "\n",
    "def corrective_actions_node(state: EvalState) -> EvalState:\n",
    "    \"\"\"Node 1: call CorrectiveActionsAgent to generate corrective actions.\"\"\"\n",
    "    pptx_text = \"\\n\\n---\\n\\n\".join(state[\"pptx_chunks\"])\n",
    "    user_prompt = state[\"user_prompt\"]\n",
    "\n",
    "    result = corrective_actions_executor.invoke(\n",
    "        {\n",
    "            \"user_prompt\": user_prompt,\n",
    "            \"pptx_text\": pptx_text,\n",
    "        }\n",
    "    )\n",
    "    raw = result[\"output\"]\n",
    "\n",
    "    try:\n",
    "        parsed = json.loads(raw)\n",
    "        actions = parsed.get(\"corrective_actions\", [])\n",
    "    except Exception:\n",
    "        actions = []\n",
    "    return {\n",
    "        \"corrective_actions_raw\": raw,\n",
    "        \"corrective_actions\": actions,\n",
    "    }\n",
    "\n",
    "def evaluation_node(state: EvalState) -> EvalState:\n",
    "    \"\"\"Node 2: call EvaluationAgent to select top-k=3 corrective actions.\"\"\"\n",
    "    corrective_actions = state[\"corrective_actions\"]\n",
    "    corrective_actions_json = json.dumps(\n",
    "        {\"corrective_actions\": corrective_actions},\n",
    "        ensure_ascii=False,\n",
    "        indent=2,\n",
    "    )\n",
    "\n",
    "    result = evaluation_executor.invoke(\n",
    "        {\"corrective_actions_json\": corrective_actions_json}\n",
    "    )\n",
    "    raw = result[\"output\"]\n",
    "\n",
    "    try:\n",
    "        parsed = json.loads(raw)\n",
    "        top_actions = parsed.get(\"top_corrective_actions\", [])\n",
    "    except Exception:\n",
    "        top_actions = []\n",
    "\n",
    "    return {\n",
    "        \"evaluated_actions_raw\": raw,\n",
    "        \"evaluated_actions\": top_actions,\n",
    "    }\n",
    "\n",
    "graph_builder = StateGraph(EvalState)\n",
    "\n",
    "graph_builder.add_node(\"parse_pptx\", parse_pptx_node)\n",
    "graph_builder.add_node(\"generate_corrective_actions\", corrective_actions_node)\n",
    "graph_builder.add_node(\"evaluate_corrective_actions\", evaluation_node)\n",
    "\n",
    "graph_builder.set_entry_point(\"parse_pptx\")\n",
    "graph_builder.add_edge(\"parse_pptx\", \"generate_corrective_actions\")\n",
    "graph_builder.add_edge(\"generate_corrective_actions\", \"evaluate_corrective_actions\")\n",
    "graph_builder.add_edge(\"evaluate_corrective_actions\", END)\n",
    "\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6460951b-e76c-43aa-9951-05b6cec9d568",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5240be7-6f7c-48be-91c5-9851c5f642a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97765fda-488f-4fed-9f8a-93c5b727c415",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# DEMO END\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69d0b6bc-dbff-4e1c-8ff4-e05ed85e200c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%writefile -a agent.py\n",
    "from typing import Annotated, Any, Generator, Optional, Sequence, TypedDict, Union\n",
    "\n",
    "import mlflow\n",
    "from databricks_langchain import (\n",
    "    ChatDatabricks,\n",
    "    UCFunctionToolkit,\n",
    "    VectorSearchRetrieverTool,\n",
    "    # DatabricksFunctionClient\n",
    ")\n",
    "from langchain_core.messages import AIMessage, AIMessageChunk, AnyMessage\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "from langchain_core.tools import BaseTool\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt.tool_node import ToolNode\n",
    "from mlflow.pyfunc import ResponsesAgent\n",
    "from mlflow.types.responses import (\n",
    "    ResponsesAgentRequest,\n",
    "    ResponsesAgentResponse,\n",
    "    ResponsesAgentStreamEvent,\n",
    "    output_to_responses_items_stream,\n",
    "    to_chat_completions_input,\n",
    ")\n",
    "from unitycatalog.ai.core.databricks import DatabricksFunctionClient\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cde80fbe-ea56-45f7-9ac2-ab7d4d455c9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Defining the Foundation Model\n",
    "As we build up our agent, we need to define the foundation model we want to back the agent. This handles all of the text en/decoding, routing logic (based on descriptions) and instruction handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c1f6877-4908-468b-93d8-407fd5c2591d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%writefile -a agent.py\n",
    "#Define the endpoint to use for the agent foundation and system prompt\n",
    "LLM_ENDPOINT_NAME = \"databricks-claude-3-7-sonnet\"\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME)\n",
    "system_prompt = \"You are a helpful assistant that can run Python code.\" #Give my agent a better description later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7c038aa-19e5-45b9-a9ff-d895d1e88bd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### AI Tools\n",
    "Various tools for agent capabilities can be added here. We're creating a collection of tools defined as `tools[]` that we then append to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93eb975f-a983-4947-9f74-a054cb2169f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%writefile -a agent.py\n",
    "tools = []\n",
    "\n",
    "# You can use UDFs in Unity Catalog as agent tools\n",
    "# Below, we add the `system.ai.python_exec` UDF, which provides\n",
    "# a python code interpreter tool to our agent\n",
    "# You can also add local LangChain python tools. See https://python.langchain.com/docs/concepts/tools\n",
    "\n",
    "# TODO: Add additional tools\n",
    "UC_TOOL_NAMES = [\"system.ai.python_exec\"]\n",
    "client = DatabricksFunctionClient(execution_mode=\"serverless\")\n",
    "uc_toolkit = UCFunctionToolkit(function_names=UC_TOOL_NAMES, client=client)\n",
    "tools.extend(uc_toolkit.tools)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86b940e7-7576-48ee-af22-031ab1acd636",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Vector Search Tools\n",
    "Vector searches and indexes can be used for context adding RAG capabilities to the agent. Databricks endpoints can be used directly using the VectorSearchRetrieverTool(). Other vectord databases can be added as external MCP endpoints. These are all appended to the `tools[]` collection we defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a8674a6-3ee3-4d81-a596-085aae1fe802",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%writefile -a agent.py\n",
    "#Vector search tools are used for unstructured text tools. Useful for a RAG agent.\n",
    "VECTOR_SEARCH_TOOLS = []\n",
    "\n",
    "# To add vector search retriever tools,\n",
    "# use VectorSearchRetrieverTool and create_tool_info,\n",
    "# then append the result to TOOL_INFOS.\n",
    "# Example:\n",
    "# VECTOR_SEARCH_TOOLS.append(\n",
    "#     VectorSearchRetrieverTool(\n",
    "#         index_name=\"\",\n",
    "#         # filters=\"...\"\n",
    "#     )\n",
    "# )\n",
    "tools.extend(VECTOR_SEARCH_TOOLS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2530a3fd-be8e-483d-8a8b-e9ad97947e33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Agent State\n",
    "`AgentState()` is an object used to persist the conversation turns of the agent. We pass this object into the conversation chain to keep track of what it's doing for the duration of the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2597b506-4a85-403b-9429-b36d0d01d9e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%writefile -a agent.py\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[AnyMessage], add_messages]\n",
    "    custom_inputs: Optional[dict[str, Any]]\n",
    "    custom_outputs: Optional[dict[str, Any]]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06b8a9bc-b2ec-4614-bd3c-668210870eec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Agent Logic\n",
    "This is where we define the actual logic for the agent and handle how the tools are called. We're also defining how the agent deals with conversation turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c8bc6c5-e4ab-4ad3-a2f3-1da699ec9ff9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%writefile -a agent.py\n",
    "def create_tool_calling_agent(\n",
    "    model: ChatDatabricks,\n",
    "    tools: Union[ToolNode, Sequence[BaseTool]],\n",
    "    system_prompt: Optional[str] = None,\n",
    "):\n",
    "    model = model.bind_tools(tools)\n",
    "\n",
    "    # Define the function that determines which node to go to\n",
    "    def should_continue(state: AgentState):\n",
    "        messages = state[\"messages\"]\n",
    "        last_message = messages[-1]\n",
    "        # If there are function calls, continue. else, end\n",
    "        if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "            return \"continue\"\n",
    "        else:\n",
    "            return \"end\"\n",
    "\n",
    "    if system_prompt:\n",
    "        preprocessor = RunnableLambda(\n",
    "            lambda state: [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "        )\n",
    "    else:\n",
    "        preprocessor = RunnableLambda(lambda state: state[\"messages\"])\n",
    "    model_runnable = preprocessor | model\n",
    "\n",
    "    def call_model(\n",
    "        state: AgentState,\n",
    "        config: RunnableConfig,\n",
    "    ):\n",
    "        response = model_runnable.invoke(state, config)\n",
    "\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    workflow = StateGraph(AgentState)\n",
    "\n",
    "    workflow.add_node(\"agent\", RunnableLambda(call_model))\n",
    "    workflow.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "    workflow.set_entry_point(\"agent\")\n",
    "    workflow.add_conditional_edges(\n",
    "        \"agent\",\n",
    "        should_continue,\n",
    "        {\n",
    "            \"continue\": \"tools\",\n",
    "            \"end\": END,\n",
    "        },\n",
    "    )\n",
    "    workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "    return workflow.compile()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "312fa90e-310b-4348-ae6f-d1018fe313aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ResponsesAgent (OpenAI) Framework\n",
    "Next, we create a new class we're calling LangGraphResponsesAgent which is a concrete implementation of the ResponsesAgent base class. ResponsesAgent is a definition created by OpenAI that's being used by pretty much every major agent platform now. This is becoming the standard implementation. We're going to be seeing future agent registries being built that require this as a protocol. ResponsesAgent is responsible for handling the conversation from human-to-agent, agent-to-agent and agent-to-human in a standard way.\n",
    "\n",
    "**IMPORTANT!** ResponsesAgent() is still classified as experimental within MLFlow - OpenAI is now considering ResponsesAgent() as stable release. MLFlow _may_ move to a different implementation later. If you prefer, you can use OpenAI's native implementation, however the MLFlow version supports the full MLOps lifecycle including conversation tracking for easy detection of hallucination.\n",
    "\n",
    "`predict()` is the boundary conversation to and from the agent. This is either the human-agent or agent-agent interface.\n",
    "`predict_stream()` is the internal conversation and discourse the agent has with itself (reasoning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6a2b009-b0ef-421c-8b11-eba7b55b9cce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%writefile -a agent.py\n",
    "class LangGraphResponsesAgent(ResponsesAgent):\n",
    "    def __init__(self, agent):\n",
    "        self.agent = agent\n",
    "\n",
    "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
    "        outputs = [\n",
    "            event.item\n",
    "            for event in self.predict_stream(request)\n",
    "            if event.type == \"response.output_item.done\"\n",
    "        ]\n",
    "        return ResponsesAgentResponse(output=outputs, custom_outputs=request.custom_inputs)\n",
    "\n",
    "    def predict_stream(\n",
    "        self,\n",
    "        request: ResponsesAgentRequest,\n",
    "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        cc_msgs = to_chat_completions_input([i.model_dump() for i in request.input])\n",
    "\n",
    "        for event in self.agent.stream({\"messages\": cc_msgs}, stream_mode=[\"updates\", \"messages\"]):\n",
    "            if event[0] == \"updates\":\n",
    "                for node_data in event[1].values():\n",
    "                    if len(node_data.get(\"messages\", [])) > 0:\n",
    "                        yield from output_to_responses_items_stream(node_data[\"messages\"])\n",
    "            # filter the streamed messages to just the generated text messages\n",
    "            elif event[0] == \"messages\":\n",
    "                try:\n",
    "                    chunk = event[1][0]\n",
    "                    if isinstance(chunk, AIMessageChunk) and (content := chunk.content):\n",
    "                        yield ResponsesAgentStreamEvent(\n",
    "                            **self.create_text_delta(delta=content, item_id=chunk.id),\n",
    "                        )\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed66fdcd-fbd8-4b38-ba1c-0d1f158312d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%writefile -a agent.py\n",
    "mlflow.langchain.autolog()\n",
    "agent = create_tool_calling_agent(llm, tools, system_prompt)\n",
    "AGENT = LangGraphResponsesAgent(agent)\n",
    "mlflow.models.set_model(AGENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f45f8191-2a65-426d-b7b4-f01fb5974dd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Testing the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fa875fb-5e72-4e0f-9fea-fd774ebd62c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Restart the python interpreter to flush out any lingering instances of in-memory objects\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd01e08c-59e1-485e-98ec-f694402bc9b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Test the summarizer\n",
    "from agent import AGENT\n",
    "\n",
    "result = AGENT.predict({\"input\": [{\"role\": \"user\", \"content\": \"What is 6*7 in Python?\"}]})\n",
    "print(result.model_dump(exclude_none=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d805b467-ca58-432f-bde7-5131a076f28f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Viewing the Results as Chunks\n",
    "Here we can blow out the response and print each chunk as it's processed by the agent. We can also clearly see how the result is being re-assembled. This is the type of conversation the agent has and we can see it's chain of reasoning to help us debug.\n",
    "\n",
    "**NOTE!** The agent description has a big effect on the results. Often logical errors or fallacies can be remedied by fixing the descriptions and instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "136ce9ad-ad9e-45fa-ac17-5fe652a366f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Test internal conversation turns\n",
    "for chunk in AGENT.predict_stream(\n",
    "    {\"input\": [{\"role\": \"user\", \"content\": \"What is 6*7 in Python?\"}]}\n",
    "):\n",
    "    print(chunk.model_dump(exclude_none=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11f21ffe-f922-49f8-814f-9ad0bceeb91f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Log the agent as an MLflow model\n",
    "__This is taken straight from the custom agent boilerplate example__\n",
    "\n",
    "Log the agent as code from the `agent.py` file (or whatever you called it in the writefile statements). See [MLflow - Models from Code](https://mlflow.org/docs/latest/models.html#models-from-code).\n",
    "\n",
    "If you are creating multiple agents, each one needs to be logged in MLFlow so it can be used later in a multi-agent setup.\n",
    "\n",
    "### Enable automatic authentication for Databricks resources\n",
    "For the most common Databricks resource types, Databricks supports and recommends declaring resource dependencies for the agent upfront **during logging**. This enables automatic authentication passthrough when you deploy the agent. With automatic authentication passthrough, Databricks automatically provisions, rotates, and manages short-lived credentials to securely access these resource dependencies from within the agent endpoint.\n",
    "\n",
    "To enable automatic authentication, specify the dependent Databricks resources when calling `mlflow.pyfunc.log_model().`\n",
    "\n",
    "  - **TODO**: If your Unity Catalog tool queries a [vector search index](docs link) or leverages [external functions](docs link), you need to include the dependent vector search index and UC connection objects, respectively, as resources. See docs ([AWS](https://docs.databricks.com/generative-ai/agent-framework/log-agent.html#specify-resources-for-automatic-authentication-passthrough) | [Azure](https://learn.microsoft.com/azure/databricks/generative-ai/agent-framework/log-agent#resources))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb2730f9-b762-4ce2-a15f-709b38766c9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Determine Databricks resources to specify for automatic auth passthrough at deployment time\n",
    "from agent import UC_TOOL_NAMES, VECTOR_SEARCH_TOOLS\n",
    "\n",
    "import mlflow\n",
    "from mlflow.models.resources import DatabricksFunction\n",
    "from pkg_resources import get_distribution\n",
    "\n",
    "#Grab all of our tool resources and add them to a list (similar to what we did with our toolbox). This will give MLFlow registry context for what tools are employed by the agent.\n",
    "resources = []\n",
    "for tool in VECTOR_SEARCH_TOOLS:\n",
    "    resources.extend(tool.resources)\n",
    "for tool_name in UC_TOOL_NAMES:\n",
    "    resources.append(DatabricksFunction(function_name=tool_name))\n",
    "\n",
    "#Take the output file (agent.py) and write it as an artifact to the MLFlow registry. Make sure to add any dependencies here as part of the pip requirements variable.\n",
    "with mlflow.start_run():\n",
    "    logged_agent_info = mlflow.pyfunc.log_model(\n",
    "        name=\"agent\",\n",
    "        python_model=\"agent.py\",\n",
    "        pip_requirements=[\n",
    "            \"databricks-langchain\",\n",
    "            f\"langgraph=={get_distribution('langgraph').version}\",\n",
    "            f\"databricks-connect=={get_distribution('databricks-connect').version}\",\n",
    "            \"uv\",\n",
    "            \"databricks-agents\"\n",
    "        ],\n",
    "        resources=resources,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61c97c85-476b-4009-ac63-834dcdd7c480",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Agent Evaluation\n",
    "Use Mosaic AI Agent Evaluation to evalaute the agent's responses based on expected responses and other evaluation criteria. Use the evaluation criteria you specify to guide iterations, using MLflow to track the computed quality metrics.\n",
    "See Databricks documentation ([AWS]((https://docs.databricks.com/aws/generative-ai/agent-evaluation) | [Azure](https://learn.microsoft.com/azure/databricks/generative-ai/agent-evaluation/)).\n",
    "\n",
    "\n",
    "To evaluate your tool calls, add custom metrics. See Databricks documentation ([AWS](https://docs.databricks.com/en/generative-ai/agent-evaluation/custom-metrics.html#evaluating-tool-calls) | [Azure](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-evaluation/custom-metrics#evaluating-tool-calls))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee583383-2320-4009-93ea-4e1ca103a77b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.genai.scorers import RelevanceToQuery, RetrievalGroundedness, RetrievalRelevance, Safety\n",
    "\n",
    "eval_dataset = [\n",
    "    {\n",
    "        \"inputs\": {\"input\": [{\"role\": \"user\", \"content\": \"Calculate the 15th Fibonacci number\"}]},\n",
    "        \"expected_response\": \"The 15th Fibonacci number is 610.\",\n",
    "    }\n",
    "]\n",
    "\n",
    "eval_results = mlflow.genai.evaluate(\n",
    "    data=eval_dataset,\n",
    "    predict_fn=lambda input: AGENT.predict({\"input\": input}),\n",
    "    scorers=[RelevanceToQuery(), Safety()],  # add more scorers here if they're applicable\n",
    ")\n",
    "\n",
    "# Review the evaluation results in the MLfLow UI (see console output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d80b8f56-daa8-44c5-9e66-d1b5616e5cd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sanity Check\n",
    "Let's do a quick evaluation of a simple prompt to make sure that MLFlow is properly hosting the agent as a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68098cb6-57c8-4f5e-a40c-d921a4b5fc38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.models.predict(\n",
    "    model_uri=f\"runs:/{logged_agent_info.run_id}/agent\",\n",
    "    input_data={\"input\": [{\"role\": \"user\", \"content\": \"What is 6*7 in Python?!\"}]},\n",
    "    env_manager=\"uv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8aa23af-43ca-4417-9830-4c4a5d56874e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Registering the Agent in UC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd7de51b-bdc5-470d-a4d3-0d97ddb8c8b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# TODO: define the catalog, schema, and model name for your UC model\n",
    "catalog = \"ademianczuk\"\n",
    "schema = \"suncor_ehs\"\n",
    "model_name = \"test_agent\"\n",
    "UC_MODEL_NAME = f\"{catalog}.{schema}.{model_name}\"\n",
    "\n",
    "# register the model to UC\n",
    "uc_registered_model_info = mlflow.register_model(model_uri=logged_agent_info.model_uri, name=UC_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ff05731-aa0f-4db3-9246-d580eea0947e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Deploy the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65b2cc89-77f5-4acd-86e5-3d3b7526c9e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "\n",
    "agents.deploy(\n",
    "    UC_MODEL_NAME,\n",
    "    uc_registered_model_info.version,\n",
    "    tags={\"endpointSource\": \"docs\"},\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Evaluation_Agent",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
