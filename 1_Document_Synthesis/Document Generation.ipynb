{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a4d0819-f121-4d69-800a-21f223a5439a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Document Generation\n",
    "In this notebook we will begin by creating several documents (probably word docs) and writing them to a databricks volume for parsing. These documents will be generated samples of corrective actions that need to be distilled based on previous output from a different system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e970cf8f-0e20-4f95-a298-ac20b770ae6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup Block\n",
    "We're going to create a number of cells that handle the setup and maintenance of the data and files for the project. Although this can be done anywhere I find that putting the configs as a dedicated section at the top of my notebook easy to manage. What matters here is the system. Since serverless pipelines don't support `%run` commands (they're declarative) it's best to decouple functions in other ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f58232c-d45a-40af-ad96-86a9b6bec83f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Dependency Installs"
    }
   },
   "outputs": [],
   "source": [
    "%pip install python-docx mlflow  --upgrade --pre\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f73e01f-51f9-4a42-a03a-b546ad40a3d4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Imports"
    }
   },
   "outputs": [],
   "source": [
    "#Python utilities\n",
    "import os, json, uuid, requests, datetime, re\n",
    "from docx import Document\n",
    "from pathlib import Path\n",
    "\n",
    "#Spark utilities\n",
    "from pyspark.sql import functions as F, types as T\n",
    "\n",
    "#Databricks utilities\n",
    "from dbruntime.databricks_repl_context import get_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa71e807-735e-4006-85a9-2422f5ececc3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get Environment Variables"
    }
   },
   "outputs": [],
   "source": [
    "#Set the variables for the PAT in the Databricks Secrets store\n",
    "secret_scope_name = \"general\"\n",
    "secret_key_name = \"genie_access\"\n",
    "\n",
    "#Inject the variables into the agent for use\n",
    "os.environ[\"DB_MODEL_SERVING_HOST_URL\"] = \"https://\" + get_context().workspaceUrl\n",
    "assert os.environ[\"DB_MODEL_SERVING_HOST_URL\"] is not None\n",
    "\n",
    "#Inject the databricks personal access token for use\n",
    "os.environ[\"DATABRICKS_GENIE_PAT\"] = dbutils.secrets.get(\n",
    "    scope=secret_scope_name, key=secret_key_name\n",
    ")\n",
    "assert os.environ[\"DATABRICKS_GENIE_PAT\"] is not None, (\n",
    "    \"The DATABRICKS_GENIE_PAT was not properly set to the PAT secret\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61e7d1f4-3969-4c34-8493-f8811897a65c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set Operating Variables and Locations"
    }
   },
   "outputs": [],
   "source": [
    "#Keeping this separate makes it easy to find - it's the most likely to need to get updated on a frequent basis\n",
    "catalog = \"ademianczuk\"\n",
    "db = \"suncor_ehs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b770fdd-c5b8-49d0-afb5-b394e2d9f2d9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configs & Directory Assignment"
    }
   },
   "outputs": [],
   "source": [
    "#Set the operating environment details\n",
    "DATABRICKS_HOST = os.environ.get(\"DB_MODEL_SERVING_HOST_URL\")\n",
    "DATABRICKS_TOKEN = os.environ.get(\"DATABRICKS_GENIE_PAT\")\n",
    "FM_ENDPOINT = \"llama-3-70b-instruct\"  # your foundation model endpoint name\n",
    "\n",
    "#Set the storage details\n",
    "VOL_ROOT = f\"/Volumes/{catalog}/{db}/data\"\n",
    "DOC_OUT_DIR = f\"{VOL_ROOT}/docs\"\n",
    "Path(DOC_OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#Set the table details\n",
    "SCENARIO_TABLE = f\"{catalog}.{db}.scenarios\"\n",
    "DOCS_TABLE = f\"{catalog}.{db}.docs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38a85b77-4344-4e64-b193-fec30dd911c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Helper Functions\n",
    "The two helper functions assist us in creating the contents for the document as well as writing them out to a storage volume. We may want to adjust the format of the word file to include tables as the real output will likely have. The point of this is to be able to reuse or extend this logic later. Since we may have a sample document, modelling off of that might be useful.\n",
    "\n",
    "### Decoupling Helper Functionality\n",
    "This might be worth porting over to a class file depending on how often it needs to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61445503-26ae-49cb-a879-2aa25a974c40",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create the Helper Functions"
    }
   },
   "outputs": [],
   "source": [
    "#These operations are functionalized to make recall easier. We are leveraging globally assigned variables here. If we decouple this, the variables will need to be added to the signature and class constructor.\n",
    "\n",
    "def call_chat(messages, temperature=0.2, max_tokens=1200):\n",
    "    \"\"\"\n",
    "    Calls Databricks Foundation Model endpoint (chat-style).\n",
    "    API schema per Foundation Model REST API docs.\n",
    "    \"\"\"\n",
    "    url = f\"{DATABRICKS_HOST}/api/2.0/serving-endpoints/{FM_ENDPOINT}/invocations\"\n",
    "    headers = {\"Authorization\": f\"Bearer {DATABRICKS_TOKEN}\", \"Content-Type\": \"application/json\"}\n",
    "    payload = {\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens\n",
    "    }\n",
    "    r = requests.post(url, headers=headers, data=json.dumps(payload), timeout=120)\n",
    "    r.raise_for_status()\n",
    "    resp = r.json()\n",
    "    \n",
    "    # Databricks FM APIs mirror OpenAI-like schema; adjust if your endpoint returns a different shape.\n",
    "    # Try common fields first:\n",
    "    content = None\n",
    "    if isinstance(resp, dict):\n",
    "        # 'choices' structure\n",
    "        choices = resp.get(\"choices\")\n",
    "        if choices and len(choices) > 0:\n",
    "            msg = choices[0].get(\"message\") or {}\n",
    "            content = msg.get(\"content\")\n",
    "    \n",
    "    return content or str(resp)\n",
    "\n",
    "def write_docx(title, actions_md, out_path):\n",
    "    doc = Document()\n",
    "    doc.add_heading(title, level=1)\n",
    "    \n",
    "    for line in actions_md.splitlines():\n",
    "        if line.strip().startswith(\"- \"):\n",
    "            doc.add_paragraph(line.strip()[2:], style=None)\n",
    "        else:\n",
    "            doc.add_paragraph(line)\n",
    "    \n",
    "    doc.save(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "581c7779-ca09-4252-a0cc-f3c06e16bdc4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Seed Scenarios"
    }
   },
   "outputs": [],
   "source": [
    "# Option A: manually seed 10–15 \"circumstances\"\n",
    "manual_scenarios = [\n",
    "    {\"scenario_id\": str(uuid.uuid4()), \"title\": \"CNC mill overheating at low spindle speeds\",\n",
    "     \"context\": {\"machine\":\"CNC mill\",\"symptoms\":[\"process temp rising\",\"reduced coolant flow\"],\"env\":\"ambient 32°C\",\n",
    "                 \"likely_modes\":[\"Heat Dissipation Failure (HDF)\"]}},\n",
    "    {\"scenario_id\": str(uuid.uuid4()), \"title\": \"Assembly press showing increased vibration\",\n",
    "     \"context\": {\"machine\":\"hydraulic press\",\"symptoms\":[\"vibration 2x baseline\",\"oil temp high\"],\"env\":\"normal\",\n",
    "                 \"likely_modes\":[\"Component Wear\",\"Hydraulic cavitation\"]}},\n",
    "    # ...add ~10-15\n",
    "]\n",
    "\n",
    "spark.createDataFrame(\n",
    "    [(s[\"scenario_id\"], s[\"title\"], json.dumps(s[\"context\"]), \"manual\") for s in manual_scenarios],\n",
    "    schema=\"scenario_id string, title string, context_json string, source string\"\n",
    ").write.mode(\"overwrite\").saveAsTable(SCENARIO_TABLE)\n",
    "\n",
    "# Option B: (optional) Add synthetic from AI4I 2020 if you've loaded it to a table\n",
    "# ai4i_df = spark.read.csv(f\"{VOL_ROOT}/ai4i_2020.csv\", header=True, inferSchema=True)\n",
    "# # Create a few diverse conditions as scenarios:\n",
    "# synth = (ai4i_df\n",
    "#          .withColumn(\"scenario_id\", F.expr(\"uuid()\"))\n",
    "#          .withColumn(\"title\", F.concat(F.lit(\"AI4I condition: setting=\"), F.col(\"Type\")))\n",
    "#          .withColumn(\"context_json\", F.to_json(F.struct(*ai4i_df.columns)))\n",
    "#          .withColumn(\"source\",\"ai4i_sample\")\n",
    "#          .limit(10))\n",
    "# synth.write.mode(\"append\").saveAsTable(SCENARIO_TABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f24a435f-fedd-49a1-9390-cd0d8569713b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Produce Corrective Actions Document"
    }
   },
   "outputs": [],
   "source": [
    "gen_system = \"\"\"You are a maintenance reliability engineer.\n",
    "Produce a detailed, **actionable** corrective action plan for the given machinery circumstance.\n",
    "Write steps that a qualified technician can execute, with parts, tools, checks, and acceptance criteria.\n",
    "Return:\n",
    "- A 2–3 paragraph context summary.\n",
    "- A prioritized list of corrective actions (10–20 items), each with:\n",
    "  - Rationale\n",
    "  - Skill level (Apprentice/Tech/Senior)\n",
    "  - Est. downtime saved (hours)\n",
    "  - Est. cost band ($, $$, $$$)\n",
    "  - Safety notes\n",
    "Finish with a short 'Verification & Re-start Procedure' checklist.\n",
    "Use neutral, professional tone.\"\"\"\n",
    "\n",
    "scenarios_df = spark.table(SCENARIO_TABLE).limit(15).collect()\n",
    "outputs = []\n",
    "for row in scenarios_df:\n",
    "    ctx = json.loads(row.context_json)\n",
    "    user_prompt = f\"\"\"Circumstance Title: {row.title}\n",
    "Context JSON:\\n{json.dumps(ctx, indent=2)}\"\"\"\n",
    "\n",
    "    content = call_chat([\n",
    "        {\"role\":\"system\",\"content\": gen_system},\n",
    "        {\"role\":\"user\",\"content\": user_prompt}\n",
    "    ], temperature=0.2, max_tokens=1600)\n",
    "\n",
    "    file_name = f\"{row.title.lower().replace(' ','_')[:60]}_{row.scenario_id[:8]}.docx\"\n",
    "    file_path = f\"{DOC_OUT_DIR}/{file_name}\"\n",
    "    write_docx(row.title, content, file_path)\n",
    "\n",
    "    outputs.append((row.scenario_id, row.title, file_path, datetime.datetime.utcnow().isoformat(), content))\n",
    "\n",
    "spark.createDataFrame(outputs, \"scenario_id string, title string, docx_path string, created_utc string, raw_text string\")\\\n",
    "     .write.mode(\"overwrite\").saveAsTable(DOCS_TABLE)\n",
    "\n",
    "print(f\"Generated {len(outputs)} documents → {DOC_OUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aac5f305-c3d5-425b-863c-8a79f2b5125b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Rank by Impact"
    }
   },
   "outputs": [],
   "source": [
    "sum_system = \"\"\"You are an expert maintenance prioritization analyst.\n",
    "Given a corrective-action document, extract each action and score it using this rubric:\n",
    "1) RiskReduction (0-5), 2) DowntimeAvoided (0-5), 3) CostEffectiveness (0-5), 4) TimeToImplement (0-5, invert score so faster=5), 5) Repeatability (0-5).\n",
    "Compute ImpactScore = 0.35*RiskReduction + 0.25*DowntimeAvoided + 0.20*CostEffectiveness + 0.10*TimeToImplement + 0.10*Repeatability.\n",
    "Return JSON with fields: actions: [{title, justification, scores:{...}, ImpactScore}], plus a brief summary.\"\"\"\n",
    "\n",
    "docs = spark.table(DOCS_TABLE).collect()\n",
    "rank_rows = []\n",
    "for d in docs:\n",
    "    summary = call_chat([\n",
    "        {\"role\":\"system\",\"content\": sum_system},\n",
    "        {\"role\":\"user\",\"content\": d.raw_text[:120000]}  # keep under context window\n",
    "    ], temperature=0.0, max_tokens=1200)\n",
    "\n",
    "    # Optional: light validation\n",
    "    m = re.search(r\"\\{.*\\}\", summary, flags=re.S)\n",
    "    json_blob = m.group(0) if m else \"{}\"\n",
    "\n",
    "    rank_rows.append((d.scenario_id, d.title, d.docx_path, json_blob))\n",
    "\n",
    "schema = \"scenario_id string, title string, docx_path string, ranking_json string\"\n",
    "spark.createDataFrame(rank_rows, schema).write.mode(\"overwrite\").saveAsTable(\"main.corrective_actions.rankings\")\n",
    "\n",
    "print(\"Ranking complete. Inspect table main.corrective_actions.rankings for top actions per doc.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Document Generation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
