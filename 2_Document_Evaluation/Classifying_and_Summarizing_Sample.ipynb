{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "794f6109-6a6c-4a45-9688-911f6fbbde8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install python-docx mlflow  --upgrade --pre\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47911634-57d2-42d8-971b-2d3aca4305ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Python utilities\n",
    "import os, json, uuid, requests, datetime, re\n",
    "from docx import Document\n",
    "from pathlib import Path\n",
    "\n",
    "#Spark utilities\n",
    "from pyspark.sql import functions as F, types as T\n",
    "\n",
    "#Databricks utilities\n",
    "from dbruntime.databricks_repl_context import get_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffa8a0e5-b193-493f-a662-9ec69928a161",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Set the variables for the PAT in the Databricks Secrets store\n",
    "secret_scope_name = \"general\"\n",
    "secret_key_name = \"genie_access\"\n",
    "\n",
    "#Inject the variables into the agent for use\n",
    "os.environ[\"DB_MODEL_SERVING_HOST_URL\"] = \"https://\" + get_context().workspaceUrl\n",
    "assert os.environ[\"DB_MODEL_SERVING_HOST_URL\"] is not None\n",
    "\n",
    "#Inject the databricks personal access token for use\n",
    "os.environ[\"DATABRICKS_GENIE_PAT\"] = dbutils.secrets.get(\n",
    "    scope=secret_scope_name, key=secret_key_name\n",
    ")\n",
    "assert os.environ[\"DATABRICKS_GENIE_PAT\"] is not None, (\n",
    "    \"The DATABRICKS_GENIE_PAT was not properly set to the PAT secret\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "810c6d63-8859-4ca6-bdbf-9ad877f819c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Keeping this separate makes it easy to find - it's the most likely to need to get updated on a frequent basis\n",
    "catalog = \"ademianczuk\"\n",
    "db = \"suncor_ehs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "251d5e79-ee7b-449c-ba8b-28a5f867d2f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Set the operating environment details\n",
    "DATABRICKS_HOST = os.environ.get(\"DB_MODEL_SERVING_HOST_URL\")\n",
    "DATABRICKS_TOKEN = os.environ.get(\"DATABRICKS_GENIE_PAT\")\n",
    "FM_ENDPOINT = \"databricks-llama-4-maverick\"  # your foundation model endpoint name\n",
    "\n",
    "#Set the storage details\n",
    "VOL_ROOT = f\"/Volumes/{catalog}/{db}/data\"\n",
    "DOC_OUT_DIR = f\"{VOL_ROOT}/docs\"\n",
    "Path(DOC_OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#Set the table details\n",
    "SCENARIO_TABLE = f\"{catalog}.{db}.scenarios\"\n",
    "DOCS_TABLE = f\"{catalog}.{db}.docs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b23267b3-7de3-48b3-aa80-aa14a979a838",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import traceback\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "def call_chat(messages, temperature=0.2, max_tokens=1200):\n",
    "    \"\"\"\n",
    "    Calls Databricks Foundation Model endpoint (chat-style).\n",
    "    API schema per Foundation Model REST API docs.\n",
    "    \"\"\"\n",
    "    # url = f\"{DATABRICKS_HOST}/api/2.0/serving-endpoints/{FM_ENDPOINT}/invocations\"\n",
    "    url = \"https://dbc-9c7dbe12-0a2f.cloud.databricks.com/serving-endpoints/databricks-llama-4-maverick/invocations\"\n",
    "    headers = {\"Authorization\": f\"Bearer {DATABRICKS_TOKEN}\", \"Content-Type\": \"application/json\"}\n",
    "    payload = {\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens\n",
    "    }\n",
    "    r = requests.post(url, headers=headers, data=json.dumps(payload), timeout=120)\n",
    "    r.raise_for_status()\n",
    "    resp = r.json()\n",
    "    \n",
    "    # Databricks FM APIs mirror OpenAI-like schema; adjust if your endpoint returns a different shape.\n",
    "    # Try common fields first:\n",
    "    content = None\n",
    "    if isinstance(resp, dict):\n",
    "        # 'choices' structure\n",
    "        choices = resp.get(\"choices\")\n",
    "        if choices and len(choices) > 0:\n",
    "            msg = choices[0].get(\"message\") or {}\n",
    "            content = msg.get(\"content\")\n",
    "    \n",
    "    return content or str(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17904c42-2cb3-4dcf-afd2-0e35db1f9daa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sum_system = \"\"\"You are an expert maintenance prioritization analyst.\n",
    "Given a corrective-action document, extract each action and score it using this rubric:\n",
    "1) RiskReduction (0-5), 2) DowntimeAvoided (0-5), 3) CostEffectiveness (0-5), 4) TimeToImplement (0-5, invert score so faster=5), 5) Repeatability (0-5).\n",
    "Compute ImpactScore = 0.35*RiskReduction + 0.25*DowntimeAvoided + 0.20*CostEffectiveness + 0.10*TimeToImplement + 0.10*Repeatability.\n",
    "Return JSON with fields: actions: [{title, justification, scores:{...}, ImpactScore}], plus a brief summary.\"\"\"\n",
    "\n",
    "docs = spark.table(DOCS_TABLE).collect()\n",
    "rank_rows = []\n",
    "for d in docs:\n",
    "    summary = call_chat([\n",
    "        {\"role\":\"system\",\"content\": sum_system},\n",
    "        {\"role\":\"user\",\"content\": d.raw_text[:120000]}  # keep under context window\n",
    "    ], temperature=0.5, max_tokens=5000)\n",
    "\n",
    "    # Optional: light validation\n",
    "    m = re.search(r\"\\{.*\\}\", summary, flags=re.S)\n",
    "    json_blob = m.group(0) if m else \"{}\"\n",
    "\n",
    "    rank_rows.append((d.scenario_id, d.title, d.docx_path, json_blob, summary))\n",
    "\n",
    "schema = \"scenario_id string, title string, docx_path string, ranking_json string, summary string\"\n",
    "spark.createDataFrame(rank_rows, schema).write.mode(\"overwrite\").saveAsTable(f\"{catalog}.{db}.rankings\")\n",
    "\n",
    "print(f\"Ranking complete. Inspect table {catalog}.{db}.rankings for top actions per doc.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b0b09ac-35d1-4eff-8e66-7ba7f67d36e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "content = call_chat([\n",
    "    {\"role\":\"system\",\"content\": \"You are a general purpose assistant. You will answer truthfully and completely regarding the source and origin of where your data comes from and what is available.\"},\n",
    "    {\"role\":\"user\",\"content\": \"Where are you getting the remaining context for the AI4I 2020 data? When I look at the dataset I have it only consists of maybe 14 columns with no descriptors. How are you generating the context, corrective and actions based on this data?\"}\n",
    "], temperature=0.2, max_tokens=5000)\n",
    "\n",
    "print(content)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Classifying_and_Summarizing_Sample",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
